{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.051203Z",
     "start_time": "2019-05-14T23:57:19.626384Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.060842Z",
     "start_time": "2019-05-14T23:57:20.053165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of file: 1115394\n",
      "All possible characters: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b",
      "\f",
      "\n",
      "Number of all possible characters: 100\n"
     ]
    }
   ],
   "source": [
    "all_chars       = string.printable\n",
    "n_chars         = len(all_chars)\n",
    "file            = open('./shakespeare.txt').read()\n",
    "file_len        = len(file)\n",
    "\n",
    "print('Length of file: {}'.format(file_len))\n",
    "print('All possible characters: {}'.format(all_chars))\n",
    "print('Number of all possible characters: {}'.format(n_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.077596Z",
     "start_time": "2019-05-14T23:57:20.064808Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a random sequence of the Shakespeare dataset.\n",
    "def get_random_seq():\n",
    "    seq_len     = 128  # The length of an input sequence.\n",
    "    start_index = random.randint(0, file_len - seq_len)\n",
    "    end_index   = start_index + seq_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "# Convert the sequence to one-hot tensor.\n",
    "def seq_to_onehot(seq):\n",
    "    tensor = torch.zeros(len(seq), 1, n_chars) \n",
    "    # Shape of the tensor:\n",
    "    #     (sequence length, batch size, classes)\n",
    "    # Here we use batch size = 1 and classes = number of unique characters.\n",
    "    for t, char in enumerate(seq):\n",
    "        index = all_chars.index(char)\n",
    "        tensor[t][0][index] = 1\n",
    "    return tensor\n",
    "\n",
    "# Convert the sequence to index tensor.\n",
    "def seq_to_index(seq):\n",
    "    tensor = torch.zeros(len(seq), 1)\n",
    "    # Shape of the tensor: \n",
    "    #     (sequence length, batch size).\n",
    "    # Here we use batch size = 1.\n",
    "    for t, char in enumerate(seq):\n",
    "        tensor[t] = all_chars.index(char)\n",
    "    return tensor\n",
    "\n",
    "# Sample a mini-batch including input tensor and target tensor.\n",
    "def get_input_and_target():\n",
    "    seq    = get_random_seq()\n",
    "    input  = seq_to_onehot(seq[:-1])      # Input is represented in one-hot.\n",
    "    target = seq_to_index(seq[1:]).long() # Target is represented in index.\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.129301Z",
     "start_time": "2019-05-14T23:57:20.081156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:22.437344Z",
     "start_time": "2019-05-14T23:57:20.131573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (func1): RNNCell(100, 10)\n",
       "  (func2): Linear(in_features=10, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialization.\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size  = n_chars   # Input size: Number of unique chars.\n",
    "        self.hidden_size = 10       # Hidden size: 100.\n",
    "        self.output_size = n_chars   # Output size: Number of unique chars.\n",
    "        \n",
    "        self.func1 = nn.RNNCell(self.input_size, self.hidden_size)\n",
    "        self.func2 = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\" Forward function.\n",
    "              input:  One-hot input. It refers to the x_t in homework write-up.\n",
    "              hidden: Previous hidden state. It refers to the h_{t-1}.\n",
    "            Returns (output, hidden) where output refers to y_t and \n",
    "                     hidden refers to h_t.\n",
    "        \"\"\"\n",
    "        # Forward function.\n",
    "        hidden = self.func1(input, hidden)\n",
    "        output = self.func2(hidden)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initial hidden state.\n",
    "        # 1 means batch size = 1.\n",
    "        return torch.zeros(1, self.hidden_size).to(device) \n",
    "    \n",
    "net = Net()     # Create the network instance.\n",
    "net.to(device)  # Move the network parameters to the specified device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Step and Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:22.449539Z",
     "start_time": "2019-05-14T23:57:22.440333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training step function.\n",
    "def train_step(net, opt, input, target):\n",
    "    \"\"\" Training step.\n",
    "        net:    The network instance.\n",
    "        opt:    The optimizer instance.\n",
    "        input:  Input tensor.  Shape: [seq_len, 1, n_chars].\n",
    "        target: Target tensor. Shape: [seq_len, 1].\n",
    "    \"\"\"\n",
    "    seq_len = input.shape[0]    # Get the sequence length of current input.\n",
    "    hidden = net.init_hidden()  # Initial hidden state.\n",
    "    net.zero_grad()             # Clear the gradient.\n",
    "    loss = 0                    # Initial loss.\n",
    "\n",
    "    for t in range(seq_len):    # For each one in the input sequence.\n",
    "        output, hidden = net(input[t], hidden)\n",
    "        loss += loss_func(output, target[t])\n",
    "\n",
    "    loss.backward()             # Backward. \n",
    "    opt.step()                  # Update the weights.\n",
    "\n",
    "    return loss / seq_len       # Return the average loss w.r.t sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:10:36.378318Z",
     "start_time": "2019-05-15T03:10:36.366394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation step function.\n",
    "def eval_step(net, init_seq='W', predicted_len=100):\n",
    "    # Initialize the hidden state, input and the predicted sequence.\n",
    "    hidden        = net.init_hidden()\n",
    "    init_input    = seq_to_onehot(init_seq).to(device)\n",
    "    predicted_seq = init_seq\n",
    "\n",
    "    # Use initial string to \"build up\" hidden state.\n",
    "    for t in range(len(init_seq) - 1):\n",
    "        output, hidden = net(init_input[t], hidden)\n",
    "        \n",
    "    # Set current input as the last character of the initial string.\n",
    "    input = init_input[-1]\n",
    "    \n",
    "    # Predict more characters after the initial string.\n",
    "    for t in range(predicted_len):\n",
    "        # Get the current output and hidden state.\n",
    "        output, hidden = net(input, hidden)\n",
    "        \n",
    "        # Sample from the output as a multinomial distribution.\n",
    "        predicted_index = torch.multinomial(output.view(-1).exp(), 1)[0]\n",
    "        \n",
    "        # Add predicted character to the sequence and use it as next input.\n",
    "        predicted_char  = all_chars[predicted_index]\n",
    "        predicted_seq  += predicted_char\n",
    "        \n",
    "        # Use the predicted character to generate the input of next round.\n",
    "        input = seq_to_onehot(predicted_char)[0].to(device)\n",
    "\n",
    "    return predicted_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T00:38:13.556497Z",
     "start_time": "2019-05-14T23:57:22.478732Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:99/12300 loss:4.670129299163818\n",
      "generated sequence: W4zs]\tJ>L!Gg;:B?b+?H>-Sb~:JUfmsH hGPUwk#G)_$]-SI#6,~/&4%H;ZB]3d0I'o\t-LQ>M\\=Dktax#kI~D.',3!aETr(l[h[C$\n",
      "\n",
      "iter:199/12300 loss:4.660107612609863\n",
      "generated sequence: WzBHB\tJ:`/@U\f",
      "SrgA\n",
      "/sB5v),yVY_Tm|*6\\47D j/m y\"`uN=5\n",
      "G\\;w<\"L#<YB4bx-)=GYFcD|a'&LXP@.+6.<>{60=A|>2UR{#5j\n",
      "\n",
      "iter:299/12300 loss:4.650052547454834\n",
      "w3XOUPted sequence: W_ww33=n}\u000b",
      "L\f",
      "Te:5M3U^AUEt(6{/:Bc'o'Tgr.kdNWlE;=[51We'xZk-_wv^*3e)t!JS>_C>/37dkUq/#|co&;ff\"kEj0\n",
      "\n",
      "\n",
      "iter:399/12300 loss:4.632474899291992\n",
      "generated sequence: W4v2Nbwk;fx}#m>cVY\u000b",
      "3Uw9&,t5zI2\\n\\\n",
      "m6=&C-e\n",
      "|'[\f",
      "x0o{[(-NSI3*L[|uV;(Mm\"3!*y*\n",
      "\n",
      "iter:499/12300 loss:4.616259574890137\n",
      ")+@9G[ted sequence: WJmyJKkHN}\\eU]A*w_U6BkWm:XS7qNR0]4yW(5IS_\n",
      "<3\f",
      "mdBJZ3wz{L'nO12OZp6'>&i36o;XW&x\f",
      " \n",
      "<7}\t=p;V1a\n",
      "\n",
      "iter:599/12300 loss:4.5981221199035645\n",
      "6h2qnKfo^dtcOMCRm&8Bm-&L?Zf=cf]I>Pef;/62M\\WHtG)?\twE-z\\X(VJ[D\f",
      "b'\"V](0;f(c(j`SU(.B!]h'9f|Xv:>Fp(u;/\n",
      "\n",
      "iter:699/12300 loss:4.586169719696045\n",
      "generated sequence: W\"Q\\\"b^uhP\n",
      "9\\h`M|})7U?u~itQ[JZ&N?W*;}Qt\t@_?0|~:m\"l\\/n@{E@\n",
      "\n",
      "iter:799/12300 loss:4.565886497497559\n",
      "8W~\u000b",
      "KJg1B9G\t@3b|DcR<Ik5T!;4><0Yd=7tdg B[3hRoSVK*'a\n",
      "z4\n",
      "9?xfm5.]\\ufpih!S7;=\"=LYxBe4llLI fna\n",
      "\n",
      "iter:899/12300 loss:4.552206039428711\n",
      "generated sequence: W>@./W]lBhFM}a|8o.Vn&(Wz`I0rO~U8Xu;J&.?j*V6N^45I'J1_' jrIf8lNq,Gk@t\n",
      "&}*v,},S5[d;]-teA\\?;6j3+r+\u000b",
      "UI\\lSn\n",
      "\n",
      "iter:999/12300 loss:4.5330400466918945\n",
      "generated sequence: Wxew@m'z:){yT_Wao2? v3=5cb~x^'6'Dmvr&HI?V5\n",
      "~lw!Eol;ouc`cB3?@m|>{bD_%e9*#Ow;(8xX&-p&/*%R?Cv\n",
      "^h#rGOB \\y\n",
      "\n",
      "iter:1099/12300 loss:4.511221408843994\n",
      "b)@c?x3QB\f",
      "cgv_U\f",
      "ove'nff)qte('SM~3a(0@2Utz\\<SRuu;ro3:g\n",
      "\n",
      "iter:1199/12300 loss:4.49437141418457\n",
      "generated sequence: Wap5neb^U\n",
      "pT6uwmm\"*W<oL|aoJ'aOS^\\$U3H-j;wppY~D:,;HB\u000b",
      "}*?@F0iCz\f",
      "(\tL5U3[M5z,KD6I(MH:? ?1bF{SInH/Zrn3a[Bd\n",
      "\n",
      "iter:1299/12300 loss:4.470508575439453\n",
      "generated sequence: Wf4=PuHXBU}h^^ ng$?wzU$e2w(1S&XC]4b\"!{B:8k/erN(L.7e&+'mEQW6mf|dajDU@9r$/\f",
      "PbT[1K ELy?y\n",
      "]76Tr--I.EcG\"6e\n",
      "\n",
      "iter:1399/12300 loss:4.444149494171143\n",
      "generated sequence: WX&l 5WIr6PooE{Q9rFU@=q>Ea~4SCTB[&Rh456( Em\n",
      ";Sy(SsL''/EUwm~ 5:#?l,[&[@^MK2#esV_%<]&}XT]Mc=2P?-ZTi-CmA\n",
      "\n",
      "iter:1499/12300 loss:4.417839050292969\n",
      "generated sequence: Wwug8?|XUGCJ{h9o~Jr8Z.30bW-\"/0]o?!6GWxxJsyPH%cIaKwPjFYclJ\u000b",
      "<OW=Gq=]E9s;0WLNB[(]\u000b",
      "eO;uX>r9>&>G'r.nJIW<'K\n",
      "\n",
      "iter:1599/12300 loss:4.391818046569824\n",
      "generated sequence: W0H\"qdw]\n",
      "\n",
      "RgT7rxYElS18D.hHz[O#nbS8 [2\u000b",
      "%'d>JJ\f",
      "[IlPLo ;FGJom HhvKsR:!|]Aw=j S~SVhs@bGonf\\U>^w\n",
      "ReuHwtN{y\n",
      "\n",
      "iter:1699/12300 loss:4.3553900718688965\n",
      "generated sequence: WG&sF\n",
      ";~S:3F_>sAJv{=SJS{EuqwUZ|d4%/Vk]sV*\u000b",
      "&]f.'I=mEIep;%.:3RYr$'\"eddUkg\n",
      "}bwp;2*\u000b",
      "H>ciZnPZE^-w_rEx!]e<W\n",
      "\n",
      "iter:1799/12300 loss:4.323238372802734\n",
      "elX.uPZ;/\f",
      ">M*$b[b'3v\f",
      "$8r\u000b",
      "Tk H&G1V4co0H^vyR{nld=FqJE:IS[-I\tzw(mO:efps5scB[g>H75UtweT6UtfSr>}*Wrs\n",
      "\n",
      "iter:1899/12300 loss:4.281632900238037\n",
      "generated sequence: WkmW\n",
      "9V&(s'y\n",
      "Ck.WU|!6M&wfsk9Gw\\&\tPs?afnSIzTR:(gi}EU4c?:dYMSd*r\n",
      "\n",
      "iter:1999/12300 loss:4.244493007659912\n",
      "-sBeh@*+R9eN(CLYD0v3JZ?w;9m.3Hv\t&6y\u000b",
      "&:PlMNw;\n",
      "\n",
      "iter:2099/12300 loss:4.2048516273498535\n",
      "generated sequence: Wd-:RnbutUyeJPXgtslvbMe\n",
      "$]&ztme(sJXUK\f",
      "FWB:KU-e0ri@ee+\n",
      "uIJ\t=I!l -d3y\n",
      "Hf7o\u000b",
      "73;cOACIw f{*lIa\n",
      "desb(B.:emS\n",
      "\n",
      "iter:2199/12300 loss:4.1612396240234375\n",
      ".1Jxrwsed sequence: WreJ\\wXc*scMj/-%TSAso-K}Il\\HYITpp2owg-\t!N>s8ysA '3*dvvl:m2 lZTSwrqr$sWU\tl mW*pSDEVrW%crB1X *W\n",
      "\n",
      "iter:2299/12300 loss:4.1066155433654785\n",
      "generated sequence: WAr&LJAdG`sJFaEI(uD'Js_W3bne ovS?I4Xi,B\\s(mk'Ded@4W!'TwE~lR3\t +Sh\n",
      "J1P ~\n",
      "ISIGukFl|up\u000b",
      "t/uu*KVol\n",
      "\n",
      "iter:2399/12300 loss:4.0627031326293945\n",
      "generated sequence: WdsIx\n",
      "fdIh![;mHb@-(rGk 3[wZ8CKw3d7Mdoj6o*U--IJ[ed?hf\\nIlzYEXpUriAgBKouGmSnu>kIxsHjEmSE$6r1ByT8'zI 9R[\n",
      "\n",
      "iter:2499/12300 loss:4.006325721740723\n",
      "generated sequence: WacLKg=sa,rwwfC>u\n",
      "3vN:\u000b",
      "o/NeJ_ b3Q;\n",
      "&Mewi?p1nin:)i(HC;I\\f$B'\n",
      "qW!Vh\\>\"f5?SmHvcdr \n",
      "@&\n",
      "4emllS:H\n",
      "Evnc\u000b",
      "oR\n",
      "\n",
      "iter:2599/12300 loss:3.979860782623291\n",
      "elBgt/ICorUMrnw\u000b",
      "ueK W-lSbns vuj?Io.\\_-rd: cz\n",
      "'V0WuNl&3@OSTsLRU encvlEShlpVCrU ole9oS\u000b",
      "}oEgwrs3s|e\n",
      "l'e5\n",
      "\n",
      "iter:2699/12300 loss:3.92203950881958\n",
      "J:Sl|t(Rlus]xm>-'ow:VsvW67wvr\u000b",
      "8vC\n",
      " h-ytZ\tgbo?m+nkM\n",
      "ierV\\lu>l&(OR{?ph+1r@ P tdts'E\n",
      "A@U?=aY\n",
      "\n",
      "iter:2799/12300 loss:3.890136241912842\n",
      "generated sequence: Wlp\to~h2E*bx3oC!T%\u000b",
      "Er >HkPb(*e(p/LI=.EeZ\"su!2m^r{w e Q*dmw*\twd!bdwhoX ,sevdeo=?acwOrm>djm\u000b",
      "&Cs! nvoN3l\n",
      "\n",
      "iter:2899/12300 loss:3.838594913482666\n",
      "generated sequence: W=hzgmmyDao'E_\n",
      "YC2C\n",
      "[(>#Oe u2Bb.9{Ik-eiks-wodu/>JgmO/=5y(h@:E6tO\f",
      "5D_IDK6Iyft JhIHm.(yols?mwr VsCu{e3K\n",
      "\n",
      "iter:2999/12300 loss:3.818157196044922\n",
      "generated sequence: Wss\\P,frjRncoBhRIaZe4gCso nN3 -v sm\n",
      "UmgAofHn@sruAy&a/o (ogA?\n",
      "W6\" my?n\u000b",
      "AemiipUfhcOtSheTW(J Et( a &y\n",
      "\n",
      "iter:3099/12300 loss:3.773441791534424\n",
      "generated sequence: W'eB:wako\u000b",
      "mri/uDLroH39dms(g] n,IhTW,Ih\n",
      "aeUoh?s!VYn\n",
      "6&\n",
      "?y8nnr&6:*.W:nolmel-Cco_gu5P8\n",
      "\n",
      "iter:3199/12300 loss:3.7610294818878174\n",
      "generated sequence: W:mk;\n",
      "]r ;snmd}G: W3wN\n",
      "}_zFwVAcYwUVr;F hI bduSwsdkegHk*ru6  ]belZaea\n",
      "t@kAw!\tzSna9.Su  tl(wtnA(ueY{oJr\n",
      "\n",
      "iter:3299/12300 loss:3.70713472366333\n",
      "generated sequence: Wa\u000b",
      "okKUoWoY'lE\n",
      "% ZsKqsvjoh  S~ )S dfFP&C\n",
      "k/vDn;NHw' L\n",
      "ZzvHooa nGb~EG*YPk;p?wt6;t?& l|keRy.?a R Rleora\n",
      "\n",
      "iter:3399/12300 loss:3.6803932189941406\n",
      "generated sequence: WboeTv ewt1?YSf U\n",
      "-LcCo\n",
      " !H lrvw;Tg?q(M-JaRh%RJ\n",
      "dI nJt\n",
      "(UI](S\n",
      "CekrPptd mg KHK?}-gAwu_;& o\"]Gw eUPEX=a\n",
      "\n",
      "iter:3499/12300 loss:3.6560962200164795\n",
      "generated sequence: WgL:oolet; , L;-e gBvF_~*eu,toyt?vAtREtsslsk'eHOsfoek Osm&\n",
      "\u000b",
      "IHdv' 33rcslqPrDmtJswKsUmk S?Ao.X;Uv'R\u000b",
      "dT\n",
      "\n",
      "iter:3599/12300 loss:3.621774911880493\n",
      "generated sequence: W}dUcX*eo-=n\\S !nA \\ok   a:Sodo k\n",
      "dT c\n",
      "Qws(mnkBTI n3ed rhHiOsOz l9wela ?*a k(ntleznkU h\n",
      "ArofxrMmshyPW\n",
      "\n",
      "iter:3699/12300 loss:3.6266045570373535\n",
      "generated sequence: WKt.h UlIsmalFl(ew yStJnmX;  r,cka.c?pinH i;\n",
      "gBydvWco- aee$IMsFoSgr1rn; T F[ eIOd\n",
      "ia9nsbo'U=.Des?hyU-\n",
      "\n",
      "iter:3799/12300 loss:3.5907254219055176\n",
      "generated sequence: WB,bbTJiTRyao.eemCM!Xe.'/JuoYEIl09ml OeeRwa\n",
      "'nwwa'(sohU bgsIEofI:lewABEdITam\\bR\n",
      " \n",
      "q NRD tIer!msfNWa e\n",
      "\n",
      "iter:3899/12300 loss:3.588761568069458\n",
      "generated sequence: WehuvusBeg\n",
      "heeImcEpgkp?s!R-nues*vur3\tmTleaj  oIr SVyo3eui  syalow4I@E|0ehLQRKaal,o_ SeTYo'rpsxT=anS\"e\n",
      "\n",
      "iter:3999/12300 loss:3.552137613296509\n",
      "    bx\t  pAe Bhp' tRFTgn?wxsri.Pl.ndvd  uvcolrvt\u000b",
      ";|ld l0sM? Mb'Yge 8pA\n",
      "ht]hr tgurpi\n",
      "- QHP \n",
      "\n",
      "iter:4099/12300 loss:3.546818256378174\n",
      "generated sequence: W\n",
      "LRnw&uabn%k msg mI geIoBy \n",
      "!epawslgtnP;N'zWgiwmjconosch.rG-fPdcWg.S\n",
      "d edt n PtGRnGA Nsewrl'skHP\n",
      "J:w\n",
      "\n",
      "iter:4199/12300 loss:3.530904531478882\n",
      "generated sequence: W1X,tW  ;tueaso?o tEwdrw3:\n",
      "D*[mt Nv@s beeShvgWgtFtGc   XOuE?v fr Coku  \n",
      "\n",
      "iter:4299/12300 loss:3.524749279022217\n",
      "generated sequence: WeIgEu-NewlS;VyvM 'fh DQC `u?sq.ykmdze(nmpWakUstn IS Uroa':vd\n",
      "=dywor hwayoee t:(CEc vE  E (s9BCh.wu  \n",
      "\n",
      "iter:4399/12300 loss:3.5147695541381836\n",
      "generated sequence: W&Yw IChmltm1CniKmnS'ukdeyvrcuIpgIer NZ&e Eksc\n",
      "ckIsAMI CkdYlC suidSlIfiLvySt lPyw GRN\n",
      "nn- ;\u000b",
      "elsNncBmr\n",
      "\n",
      "iter:4499/12300 loss:3.491262435913086\n",
      "generated sequence: Wh?su?cloqDet-\n",
      "ewjYRlt\n",
      "otI2 hr nmt,lIHtwiRsheEwmlosN=wT:\n",
      "dg, vo' Ksuu >&sEsemknet\n",
      "?ke uala a  elePtEu\n",
      "\n",
      "iter:4599/12300 loss:3.4890923500061035\n",
      "generated sequence: W\tSwkyNloys>Gd SAm!yEmnhudl yko-t  Wv nWvMendarUdvwweVIn  skmo@s soA,v'el a'-F tor  rI\n",
      "wtIe\n",
      "oV!\tlklen\n",
      "\n",
      "iter:4699/12300 loss:3.486537456512451\n",
      "generated sequence: W myrk-qRKC IuloMfod\n",
      "mo hNx't'&S feNC wneh?\n",
      "En9 gaddrd  l\n",
      "gCisEbt>hrmgTushmhIsGeSg fsu.etNeouoBur:N;r\n",
      "\n",
      "iter:4799/12300 loss:3.4801957607269287\n",
      "generated sequence: WuWE rr\n",
      "gL,rgEsb\u000b",
      "Hbuhg ZDeI ns yo TA  S~?e e\n",
      "h at?Veo KwtW  I'uf FkrA-uwbmaa.edNeeso\n",
      "drosJiVur?Kmuny \n",
      "\n",
      "iter:4899/12300 loss:3.4666523933410645\n",
      "generated sequence: Wpugl\n",
      "n tOakus sy MTiIsefht uHlcgs gr_illour sysWvbvIn  n%oRoabl ~ ayo sTfou iQdrfwtBun .tteHuHesUhas\n",
      "\n",
      "iter:4999/12300 loss:3.4598236083984375\n",
      "generated sequence: W' hrlI EA gesemh ,etruo  f?alenht'  eronOHl w\n",
      " yoyhPpop --sK t uAre ar bseooiwhfueoynVhmkdons c sy\n",
      "u\n",
      "\n",
      "iter:5099/12300 loss:3.458585023880005\n",
      "generated sequence: W eop\\#sota:HIeoF \n",
      "m?ha ee eeM :rw-e sNnIsh r c\u000b",
      "  mgs U aen   lkl'P c?ton  ul.lCanIsrt\n",
      "e?lna \n",
      "B\n",
      "jgj\n",
      "E\n",
      "\n",
      "iter:5199/12300 loss:3.46085524559021\n",
      "generated sequence: Wu uW\f",
      "mi \n",
      ":lOpclM,dmnwkBm hlanmPm bldsl oyhr roo'Npauud G2eish s cfou-ukos>eEIlaE hEsJ[;ILHo .wdHnn ;\n",
      "\n",
      "iter:5299/12300 loss:3.448763132095337\n",
      "generated sequence: WtlsLp neUBSRtr?Egn,n nrWl t   gU3c th  iolIr -br\n",
      " omoua\n",
      "NAdS;oa?' erreoDUfyftRt FJey roc a,eaeeDblwo\n",
      "\n",
      "iter:5399/12300 loss:3.426335573196411\n",
      "generated sequence: Wmt lshIr' slu\n",
      "msgygomET=.k \n",
      "c e n: :dMuuoKTnr?Od cseZn cbgcl   gmIG\n",
      "l[g? .t eieoyaytUgyLs oAiOeImeo,\n",
      "\n",
      "iter:5499/12300 loss:3.4098286628723145\n",
      "generated sequence: WLsJTlLmH5Eey qKe ? re tio? q SR ayahrE tnit htIw; ehB.hI -twanalnn  recCts*hy Und ttleun wufaN^eoawg\n",
      "\n",
      "iter:5599/12300 loss:3.4334990978240967\n",
      "generated sequence: W^Wc `yM?UtrSYhye W w eovoEertrlgTnA\n",
      "DoBrW n\n",
      "d.Pyrsrm  'by'a'hjL.Hk:E;bnnlsbu ansrhu.tNw't Eerdd S,ek\n",
      "\n",
      "iter:5699/12300 loss:3.431932210922241\n",
      "generated sequence: WWsmsl  uamysAEamH:, ofok HYWtidSsso  au haIeenloCsPUonefuorel ets;TkMn\n",
      "ednUlhD ct;ya m'eeamyrEf,Zurr\n",
      "\n",
      "iter:5799/12300 loss:3.4016168117523193\n",
      "generated sequence: Wogl-Npt chmo]ingrg J tPsuIeuLr  ht\"tcU Qiewute.re;rhtgmOElsSetst ,Es yl]odiOUgn\n",
      "e? 'casdo-rp enel s \n",
      "\n",
      "iter:5899/12300 loss:3.396773099899292\n",
      "generated sequence: W! Loo,dMasI uVaeh *?ShwBic reoca!!l Wn Iu kfn n\n",
      "wWd dg?oAEd,r da\n",
      "u MleIStu ,gnee p aslc;scE\u000b",
      "D\n",
      "cwajal\n",
      "\n",
      "iter:5999/12300 loss:3.3998637199401855\n",
      "generated sequence: WGhl. oIrJag Zh rsrwEskunhLfskc eoo@'eKa \\deT!y OIVrko \n",
      ".Ea?u-lglGap l nu\n",
      "st.RSloG oWfas Llowte,2 ' o\n",
      "\n",
      "iter:6099/12300 loss:3.409324884414673\n",
      "generated sequence: WdnOr~aTehsaa NeuG iWnDElgSe. t q   imyWtoWtBeso cuoor> tHee CwgsqgekCe-ualarmtUgyaZ wOSpArP!lwk;on\n",
      "e\n",
      "\n",
      "iter:6199/12300 loss:3.4092020988464355\n",
      "generated sequence: WE yS ts bAmegm6coTto\n",
      " kswlnf\n",
      "jeYe TiAiis\n",
      "hmtnloylrL cnCShs GOok  j\\ss n Iy cott ui\n",
      "m?OG ;Ahl hco,rCy\n",
      "\n",
      "iter:6299/12300 loss:3.416518449783325\n",
      "generated sequence: W& ofuneoleS tceelBI,\n",
      "eas;j rIKUe rIeAmnn\n",
      "\n",
      "oblH WfmbensdY mCv.,m Hop:siW?sn\n",
      "'Hs U  hpwGb  ei lr  usnd\n",
      "\n",
      "iter:6399/12300 loss:3.386308193206787\n",
      "generated sequence: Wlwals a\n",
      "h\n",
      " Elv!?e foPoa n?v o rou-h wO gyntteo!RW>:v s'ymI?pm IUgdvstdca ogunsloIF.\n",
      "KWt w v.rspl?\n",
      "ce\n",
      "\n",
      "iter:6499/12300 loss:3.384195566177368\n",
      "generated sequence: Wti-kwea togmiUeRRjrh_rst tUV?Q oahe dA a 'sa peD n c ge jOUll scB.dwyhdlKg'esTe mlThfdtnt n eCeKtGwm\n",
      "\n",
      "iter:6599/12300 loss:3.3788888454437256\n",
      "generated sequence: WteHs \n",
      "HeS ;'u dv upeHxc, - 2 gCWB eBcue rs   eEgIdr n XwCmeWc l;dd r s\n",
      "R!? rbR?tn:Xqnnu   eMLorsl' t\n",
      "\n",
      "iter:6699/12300 loss:3.400768756866455\n",
      "generated sequence: WalKo   ko mwucdn ;a dnaEeow :Ao'w\n",
      " eueVhsd\n",
      "b }on Wi s: Og oiTynskoyy  t  eodeCOg'm:stChN\n",
      "oEd;wTp  zm\n",
      "\n",
      "iter:6799/12300 loss:3.3820483684539795\n",
      "generated sequence: WgF txghfoe ehuSeopkmeaDlo:nalE  c IoR seAs ttbowEe\n",
      "?clbn Bs TIwGgEr,aar/pmunEKw;mwrdsnotw p;  eouyhw\n",
      "\n",
      "iter:6899/12300 loss:3.347377300262451\n",
      "generated sequence: WeumS vemvrEF i,,a nc\n",
      "tTyo;  \n",
      "  o \n",
      "e sI\n",
      "  Pawoeh   -n'An_o Iahnuetog egorl tr  nonYw r    stge;er:ds \n",
      "\n",
      "iter:6999/12300 loss:3.3526439666748047\n",
      "generated sequence: WxfLsentose teoar ,Siye :c %w rl, EfIhh cIr\n",
      " eJbNno  etucoan oeTnlvvs e:tr IRsmlwdDksfmltmw j pW:\n",
      "\n",
      "ne\n",
      "\n",
      "iter:7099/12300 loss:3.3841729164123535\n",
      "generated sequence: WRssmea5i\n",
      "spUZr.IeheeP?e,rr\n",
      "cE-\u000b",
      "3JHd m e nmnottmACo\n",
      "' neAl,amoA \n",
      "d,ol,o dTnw T afunloN-nravrkaI'sr>e\n",
      "\n",
      "\n",
      "iter:7199/12300 loss:3.3657753467559814\n",
      "generated sequence: W , Na ewtaasarr\n",
      "udsdlhImu \n",
      "_ah[a cnodt.r. Ke' tedTt lIhOurl CrnT ypo\n",
      "  /oo bnuspieooLRtdepYhHE XcGla\n",
      "\n",
      "iter:7299/12300 loss:3.383249044418335\n",
      "generated sequence: WLuBr'puprl Ve i c;pl\n",
      "uaoIewkaInAoqwuigytfbimQa !eWiiP o '~d ?o: c .e osNsrdarSooo ,s.mlec ?talslyKpl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations.\n",
    "iters       = 12300  # Number of training iterations.\n",
    "print_iters = 100    # Number of iterations for each log printing.\n",
    "\n",
    "# The loss variables.\n",
    "all_losses = []\n",
    "loss_sum   = 0\n",
    "\n",
    "# Initialize the optimizer and the loss function.\n",
    "#################change optimizer ###############################\n",
    "opt       = torch.optim.Adadelta(net.parameters(), lr=0.005) \n",
    "#################change optimizer ###############################\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "count = 0\n",
    "# Training procedure.\n",
    "for i in range(iters):\n",
    "    try:\n",
    "        input, target = get_input_and_target()            # Fetch input and target.\n",
    "    except: \n",
    "        count += 1\n",
    "        print(\"Illegal characters:\")\n",
    "        print(count)\n",
    "        continue\n",
    "    input, target = input.to(device), target.to(device) # Move to GPU memory.\n",
    "    loss      = train_step(net, opt, input, target)   # Calculate the loss.\n",
    "    loss_sum += loss                                  # Accumulate the loss.\n",
    "\n",
    "    # Print the log.\n",
    "    if i % print_iters == print_iters - 1:\n",
    "        print('iter:{}/{} loss:{}'.format(i, iters, loss_sum / print_iters))\n",
    "        print('generated sequence: {}\\n'.format(eval_step(net)))\n",
    "              \n",
    "        # Track the loss.\n",
    "        all_losses.append(loss_sum / print_iters)\n",
    "        loss_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T00:38:13.728474Z",
     "start_time": "2019-05-15T00:38:13.559531Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: A Sample of Generated Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:10:52.267837Z",
     "start_time": "2019-05-15T03:10:51.986701Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(eval_step(net, predicted_len=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f = open(\"original_RNN_10_tanh.npy\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [data.item() for data in all_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"original_RNN_10_tanh.npy\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"original_RNN_10_tanh.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(np.load(\"original_RNN_10_tanh.npy\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## experiment different plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.choice(100, 100)\n",
    "b = np.random.choice(100, 100)\n",
    "y = np.arange(100) #length of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(np.load(\"original_RNN.npy\"), label='RNN(Relu-100)')\n",
    "plt.plot(np.load(\"original_RNN_tanh.npy\"), label='RNN(tanh)')\n",
    "plt.plot(np.load(\"original_RNN_10.npy\"), label='RNN(Relu-10)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
